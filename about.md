---
title: Business Success with Data
layout: page
description: About
bodyClass: page-about
---

> <h2> "Data is the new oil" (Clive Humby) </h2>

Just like in fossil times, oil found in geological formations must be recovered through drilling, fracking, or alternative methods. For most use cases, the extracted oil needs to be processed and refined into various products, such as gasoline or heating oil, before it can be efficiently used.

<h3>Data is the new oil‚Äîcomparable, yet fundamentally different. </h3>

Unlike oil, we generate vast amounts of new data every day. As of now, an estimated 149 zettabytes exist‚Äîthough exact sources vary.
<!--- To put this number into perspective: One zettabyte equals one sextillion bytes (1,000,000,000,000,000,000,000 bytes) or the storage capacity of 250 billion DVDs.
-->
In other words, over 90% of the world‚Äôs data has been generated in just the past two years. The data volume doubles approximately every four years. 
<!---It does not grow as fast as Moore‚Äôs Law, which states that the number of transistors in integrated circuits doubles every 24 months but this exponential growth is comparable to a high-risk investment in tech ETFs or certain cryptocurrencies. ---> This contrasts with oil, where production rates remain relatively constant, while reserves tend to increase each year.
<!--In 1940, global oil reserves were estimated at 6 billion tons, providing a supply for just 21 years. By 2007, reserves had risen to 180 billion tons, extending the projected supply to 46 years. Even today, the estimated oil reserves are expected to last around 40 years. <a href="https://www.bveg.de/die-branche/erdgas-und-erdoel-in-deutschland/erdoelreserven-in-deutschland/">[Source]</a> 
![Data is the new oil](/images/zbynek-burival-GrmwVnVSSdU-unsplash.jpg)
<small>
Foto von <a href="https://unsplash.com/de/@zburival?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash">Zbynek Burival</a> auf <a href="https://unsplash.com/de/fotos/sonnenuntergang-GrmwVnVSSdU?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash">Unsplash</a> </small>
-->
> <h2>"Data is valuable but if unrefined it cannot be really used" (Michael Palmer)</h2>

Data without context is inherently worthless. Only through **contextualization**‚Äîby defining what a dataset represents‚Äîdoes raw data transform into **meaningful information** you can use to support your business. 

<h3> Context turns data into insights </h3>

Simply providing a data value pair like [48.6560, 9.2204] leaves its meaning unclear. Only by adding context‚Äîsuch as specifying that the pair represents geographic coordinates in decimal degrees‚Äîdoes it become an informative statement, like üìç Latitude: 48.6560¬∞ N, üìç Longitude: 9.2205¬∞ E: The geocoordinates of my hometown. 

<h3> SPDM Tech focuses on Simulation Automation and Data </h3>

For the purpose of **numerical simulations**, it is essential to convert vast quantities of raw data into **meaningful and actionable insights** - preferably well described, tool-supported and automated to satisfy credibility requirements - enabling deeper analysis and informed decision-making. 

> <h2> "Most Simulation runs hold no value" (Daniel Kr√§tschmer) </h2>

As Simulation is still an expert domain in Engineering and just **badly democratized** it suffers comparable limitation as digitalization does. The pure distance in competence and understanding between the decisive Engineering Business represented by Line Managers, Program and Project Managers and the Simulation Expert (or in analogy: IT and Software Development Experts) is often high and there is a noticeable risk that simulation activites are mainly **driven and initiated by Experts not understood by the Business**.
That might be one of the reasons why a lot of existing virtualization and simulation potential remains unleashed: It is just not connected to the Business where decisions are regularly met. 

<h3> SPDM Tech helps to contextualize simulation artefacts to foster profound decision-making  </h3>

<!---
Simulation Input Data is processed by solving the governing equations, resulting in the following data:
- Field Data (Primary Variables): Fundamental results directly computed by the solver, such as displacement, strain, stress, and temperature values.
- Derived Data (Post-Processing Results): Computed from field data to support analysis, including principal and equivalent stresses, fatigue life, damage estimation, and other failure criteria.
-->

Since simulation runs constitute only a fraction of the **complete engineering process** in e.g. automotive engineering, establishing a **seamless connection between product master data and simulation objects is** essential to ensure consistency, accuracy, and integration across the entire value chain: It is a necessary prerequisite to realize various use cases ranging from required CO2E calculations to virtual referencing and release of reliability-driven product testing. Collecting information about the device to be tested sounds simple - but in heterogeneous engineering reality it is often difficult to connect Devices under Test (DuT)-instances with transparent product master data.    

**SPDM Tech** specializes in establishing **data architectures and ontologies** for comprehensive engineering processes, with a focus on simulation data as our core element. By semantically describing both attributes and interrelations of data objects, we create **knowledge graphs** that enhance data integration and accessibility without the need to incorporate entire model information.

Knowledge graphs play a pivotal role in enhancing **tracability**, **lineage and relationships of data points** by semantically linking data from various sources. Additionally they foster AI-driven Engineering Applications providing the contextual backbone necessary for advanced data integration and analysis.

<h3> Insights require reliable data sources </h3>

Despite its importance, documentation is often the first casualty in **real-world engineering** ‚Äî just like metadata, which is frequently missing or incorrect. These errors are directly transferred into knowledge graphs, where they are amplified through data connections, ultimately leading to incorrect results in downstream applications. Simply put: the quality of your connected data sources is **mission-critical for success** in any data-driven business. So take care! 



<!---
A simulation run in structural mechanics requires several key inputs:

- Geometry Information: The structure's shape is discretized into a finite element mesh, consisting of nodes and elements.
- Material Properties: Defined based on the physical domain and type of simulation, including parameters like elasticity, plasticity, and thermal properties.
- Boundary Conditions: Constraints such as fixed supports, symmetry conditions, or prescribed displacements.
- Load Data: Applied forces, pressures, thermal loads, or other external influences affecting the structure.

![Data is the new oil](/images/2025-02-26_Simulation_Contour_plot.png)
<small> Source: Daniel Kr√§tschmer, [http://dx.doi.org/10.18419/opus-1923](http://dx.doi.org/10.18419/opus-1923)</small>

This information is processed by solving the governing equations, resulting in the following data:
- Field Data (Primary Variables): Fundamental results directly computed by the solver, such as displacement, strain, stress, and temperature values.
- Derived Data (Post-Processing Results): Computed from field data to support analysis, including principal and equivalent stresses, fatigue life, damage estimation, and other failure criteria.
 

![Data is the new oil](/images/2025-02-25_Simulation_Temperatue_overlay.png)


<small>
Image Source: <a href="https://library.dynardo.de/fileadmin/Material_Dynardo/bibliothek/WOST19/02_WOST22_optiSLang_Data_Centricity_BOSCH_Kraetschmer.pdf">Daniel Kr√§tschmer: "optiSLang as part of a data-centric ecosystem", Robert Bosch GmbH, WOST Workshop 2022</a> 

Simulation Data Management (SDM) tends to desribe various simulation artefacts:
- Simulation Models
- Simulation Analysis
- Simulation Results 





Daten ohne Kontext sind zuallererst wertlos. Erst durch eine Kontextualisierung, eine Beschreibung, was durch den Datensatz beschrieben wird, wird aus einem Datum eine Information.    




Im Gegensatz zu √ñl produzieren wir t√§glich Unmengen an neuen Daten - mittlerweile exisitieren bei nicht eindeutiger Quellenlage ca. 149 Zettabyte. um die Zahl einzuordnen: Ein Zettabyte entspricht 1 Sextillion Bytes (1.000.000.000.000.000.000.000 Bytes) oder der Speicherkapazit√§t von 250 Milliarden DVDs. 
Anders ausgedr√ºckt. √úber 90% der weltweiten Daten wurden in den letzten zwei Jahren generiert. Die Verdopplungsgeschwindigkeit betr√§gt 4 Jahre (im Gegensatz zu Moore's Law, das besagt, dass sich die Anzahl der Transistoren integrierter Schaltkreise alle 24 Monate verdoppelt). Das entspricht einem riskanten Investment in Tech-ETFs oder bei bestimmten Krytow√§hrungen. 

-->









